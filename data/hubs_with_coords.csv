,City,Name,description,Description,Media link,Tags,Hub,lat,lng
2,Oxford,AI Safety Hub,"AI Safety Hub aims to support individuals who want to contribute to the safe development of AI, and does so primarily through supporting local AI safety groups across the globe. More information can be found on their website: aisafetyhub.org



",,,,1,51.7520131,-1.2578499
3,Groningen,AI Safety Initiative Groningen,"We have a lot of AI research and education going on in Groningen. Our mission is to empower and connect university students, academics, everyone - to shape a safe and responsible future of AI. We are running the AGISF course, hackathons, talks, and additional events. More information can be found on our website: https://www.aisig.org/.

aisig.org

mailto:info@aisig.org",aisig.org,mailto:info@aisig.org,ENAIS,1,53.2190652,6.5680077
4, Amsterdam,Amsterdam,"The University of Amsterdam produces excellent AI research, including interpretability work, particularly in the ILLC and AMLab. However AIS is not widespread in the uni. One reinforcement learning researcher (Herke van Hoof) supervised two students in their safety-related master's thesis projects who then became full-time AIS researchers at the PhD student level, so there's at least some basic amount of openness toward safety work. In general, there is some amount of interest on AI Safety among university students and PhD students, and some university students read through the fundamentals program together. There is not AIS group yet. 



",,,ENAIS,1,52.3730796,4.8924534
5,Budapest,BAIST (Budapest AI Safety Team),"

https://www.budapestaisafety.org/

",https://www.budapestaisafety.org/,,ENAIS,1,47.48138955,19.14609412691246
7, Berlin,Berlin,"Irregular ""social"" discussion meetups for people to talk about AI alignment. Previously some hobbyist research, not anymore. Some research on interpretability at TUB, previously some research on safe reinforcement learning.

mailto:pranomestro@gmail.com

",mailto:pranomestro@gmail.com,,ENAIS,1,52.5170365,13.3888599
8, Brussels,Brussels,"Not a hub for technical AI safety research, but a hub for policy work. 



",,,ENAIS,1,50.8465573,4.351697
9,Cambridge,Cambridge,"

https://www.cambridgeaisafety.org/

",https://www.cambridgeaisafety.org/,,,1,52.2055314,0.1186637
10,Copenhagen,Copenhagen,"Apart Research and the Alignment Jams' headquarters are in Copenhagen. A weekly AI safety meetup happens at Bastard Café on Thursdays from 8PM. The AI Safety Reading Group and aisafety.com is also headquartered here with Søren Elverlin.

https://www.facebook.com/groups/266673130348543

",https://www.facebook.com/groups/266673130348543,,ENAIS,1,55.6867243,12.5700724
12, Delft,Delft,"We are running AGISF and regularly organising hackathons. We are planning to scale up in the next months to become a bigger student organisation, attract more people and run cool projects. There is also one professor doing AI Safety (not with a focus on AGI tho) research here. (Website/social media will be set up soon. Until then you can contact us at: jan@eadelft.org)



",,,ENAIS,1,51.999457199999995,4.362724538543995
16,Eindhoven,Eindhoven AI Safety Team,"We're setting up an AI Safety Team in collaboration with Serpentine AI. There are several independant AIS researchers in Eindhoven, as well as some interested faculty. You can contact info@eaeindhoven.nl or join the group chat!

https://www.eaeindhoven.nl/ai-safety-team

",https://www.eaeindhoven.nl/ai-safety-team,,ENAIS,1,51.4392648,5.478633
18,Tel Aviv,Israel,"ALTER Israel is working with several researchers, primarily focused on Vanessa Kosoy's Learning Theoretic Alignment, and supporting and connecting researchers in Israel with the broader safety community. We ran an AI Safety Israel conference in 2022, and are considering next steps and other opportunities. There is also a planned weekly AI-Safety coworking session at the EA Israel office. (Other locations in Israel are included as well.)



",,,,1,32.0852997,34.7818064
20,Lausanne,Lausanne,"Few research, but many interested students, some which do independent research. There is also LAIA, a student group started in 2022.

go.epfl.ch/laia

",go.epfl.ch/laia,,ENAIS,1,46.5218269,6.6327025
21,London,London,"There exists a ""Connection"" shared slack channel ""london-ai-safety-hub"" mostly maintained by Maris Sala. Invite only, message Maris (maris@conjecture.dev) with your involvement in AI Safety to join.

SERI MATS currently have a coworking space in London.
Conjecture are based in London and may host visitors on direct invite.
Center on Long-term Risk is based in London.
Deepmind has a London office.
There also exist several uni groups, EA & Rationalish meetups

maris@conjecture.dev

",maris@conjecture.dev,,,1,51.5073359,-0.12765
27, Prague,Prague,"- ACS research group
- Human-aligned Summerschool
- Prague Fall Season / FixedPoint co-working   -PIBBSS



",,,ENAIS,1,50.0874654,14.4212535
32, Tübingen,Tübingen,"Tübingen has a large cluster of AI institutes, including two Max-Planck Institutes and a large department at the university. The IMPRS-IS which is the joint program for all institutes has over 200 PhD students working on AI and related questions. According to statements by the organisers of the IMPRS-IS, Tübingen is the biggest AI location in Europe. There is a large and vibrant EA chapter in Tübingen that regularly has meetings with 20 or more people some of whom are focused on AI safety. Unfortunately, nobody in the faculty seems really interested about the kind of AI safety that tries to prevent catastrophic outcomes from misaligned AI. I (Marius) think there should be a presence of AI safety researchers there, but I'm also frustrated with the current faculty and thus intend to leave Tübingen in the near future. 



",,,ENAIS,1,48.5236164,9.0535531
35, Zurich,ZAIA,"Zürich AI Alignment group (ZAIA): (see website)
1) AGI Safety Fundamentals Course
2) Reading group discussing recent AI safety research
3) Collaboration on research projects

ETH Zürich near-term AI safety research:
1) Safe Artificial Intelligence
2) Computer Security and Privacy Group 
3) Learning & Adaptive Systems Group  

OpenPhil funded researchers:
1) Prof. Dr. Florian Tramèr (formerly OpenPhil)
2) Cynthia Chen (PhD student)
3) Zhijing Jin (PhD student)

Other researchers:
1) Lennart Heim (GovAI) 
2) Jérémy Scheurer (FAR AI)
3) Johannes Gasteiger (Google Research)

https://www.zurich-ai-alignment.com/ 

",https://www.zurich-ai-alignment.com/ ,,ENAIS,1,47.3744489,8.5410422
